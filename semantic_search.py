"""
é«˜æ€§èƒ½è¯­ä¹‰æœç´¢å¼•æ“
ä¸“ä¸ºNotionçŸ¥è¯†åº“RAGç³»ç»Ÿè®¾è®¡ï¼Œé‡‡ç”¨æœ€ä½³æ€§èƒ½å®è·µ

æ€§èƒ½ç‰¹æ€§ï¼š
- BGE/FlagEmbeddingé«˜è´¨é‡ä¸­æ–‡åµŒå…¥æ¨¡å‹
- FAISSé«˜é€Ÿå‘é‡ç´¢å¼•
- å¤šçº§ç¼“å­˜ç­–ç•¥ï¼ˆå†…å­˜+ç£ç›˜ï¼‰
- GPUåŠ é€Ÿæ”¯æŒ
- æ‰¹é‡å¤„ç†ä¼˜åŒ–
- å¼‚æ­¥æ“ä½œæ”¯æŒ
"""

import os
import json
import time
import pickle
import hashlib
import logging
from typing import List, Dict, Tuple, Optional, Union
from dataclasses import dataclass
from pathlib import Path

import numpy as np
import torch
import faiss
from sentence_transformers import SentenceTransformer
from FlagEmbedding import BGEM3FlagModel
import psutil
from diskcache import Cache
from loguru import logger

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    knowledge_id: str
    title: str
    content_snippet: str
    similarity_score: float
    source_type: str  # 'semantic', 'keyword', 'hybrid'
    metadata: Dict
    full_content: str = ""

@dataclass
class SearchConfig:
    """æœç´¢é…ç½®"""
    # æ¨¡å‹é…ç½®
    embedding_model: str = "BAAI/bge-large-zh-v1.5"  # é«˜æ€§èƒ½ä¸­æ–‡æ¨¡å‹
    device: str = "auto"  # auto/cpu/cuda
    max_seq_length: int = 512
    batch_size: int = 32
    
    # æœç´¢é…ç½®
    similarity_threshold: float = 0.3
    max_results: int = 10
    chunk_size: int = 300
    chunk_overlap: int = 50
    
    # æ€§èƒ½é…ç½®
    enable_gpu: bool = True
    enable_cache: bool = True
    cache_ttl_hours: int = 24
    enable_batch_processing: bool = True
    
    # ç´¢å¼•é…ç½®
    index_type: str = "auto"  # "flat", "ivf", "hnsw", "auto"
    nlist: int = 100  # IVFå‚æ•°
    efConstruction: int = 200  # HNSWå‚æ•°
    M: int = 16  # HNSWå‚æ•°

class HighPerformanceSemanticSearch:
    """é«˜æ€§èƒ½è¯­ä¹‰æœç´¢å¼•æ“"""
    
    def __init__(self, config: SearchConfig, cache_dir: str = "./cache"):
        self.config = config
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.index = None
        self.knowledge_items = []
        self.id_mapping = {}
        
        # ç¼“å­˜ç³»ç»Ÿ
        if config.enable_cache:
            self.disk_cache = Cache(str(self.cache_dir / "embeddings"))
            self.result_cache = Cache(str(self.cache_dir / "results"))
        else:
            self.disk_cache = None
            self.result_cache = None
        
        # æ€§èƒ½ç›‘æ§
        self.stats = {
            "total_searches": 0,
            "cache_hits": 0,
            "avg_search_time": 0.0,
            "index_size": 0
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–é«˜æ€§èƒ½è¯­ä¹‰æœç´¢å¼•æ“")
        logger.info(f"ğŸ“Š è®¾å¤‡: {self._get_device()}")
        logger.info(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")
    
    def _get_device(self) -> str:
        """æ™ºèƒ½è®¾å¤‡é€‰æ‹©"""
        if self.config.device == "auto":
            if torch.cuda.is_available() and self.config.enable_gpu:
                device = "cuda"
                logger.info(f"ğŸ® æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                device = "cpu"
                logger.info(f"ğŸ’» ä½¿ç”¨CPU: {psutil.cpu_count()}æ ¸")
        else:
            device = self.config.device
        
        return device
    
    def initialize_model(self) -> bool:
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            device = self._get_device()
            
            logger.info(f"ğŸ“¥ åŠ è½½åµŒå…¥æ¨¡å‹: {self.config.embedding_model}")
            start_time = time.time()
            
            # ä¼˜å…ˆä½¿ç”¨BGEæ¨¡å‹ï¼ˆæ›´é«˜æ€§èƒ½ï¼‰
            if "bge" in self.config.embedding_model.lower():
                self.model = BGEM3FlagModel(
                    self.config.embedding_model,
                    use_fp16=True if device == "cuda" else False,
                    device=device
                )
                self.model_type = "bge"
                logger.info("ğŸ¯ ä½¿ç”¨BGEé«˜æ€§èƒ½æ¨¡å‹")
            else:
                # å¤‡é€‰ï¼šsentence-transformers
                self.model = SentenceTransformer(
                    self.config.embedding_model,
                    device=device
                )
                self.model_type = "sentence_transformer"
                logger.info("ğŸ“ ä½¿ç”¨Sentence-Transformeræ¨¡å‹")
            
            load_time = time.time() - start_time
            logger.success(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
            
            # é¢„çƒ­æ¨¡å‹
            self._warmup_model()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _warmup_model(self):
        """æ¨¡å‹é¢„çƒ­ï¼Œä¼˜åŒ–é¦–æ¬¡æŸ¥è¯¢æ€§èƒ½"""
        logger.info("ğŸ”¥ æ¨¡å‹é¢„çƒ­ä¸­...")
        dummy_texts = ["æµ‹è¯•æ–‡æœ¬é¢„çƒ­", "æ¨¡å‹æ€§èƒ½ä¼˜åŒ–", "å‘é‡è®¡ç®—é¢„å¤„ç†"]
        
        start_time = time.time()
        _ = self.get_embeddings(dummy_texts)
        warmup_time = time.time() - start_time
        
        logger.success(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {warmup_time:.2f}ç§’")
    
    def get_embeddings(self, texts: Union[str, List[str]], show_progress: bool = False) -> np.ndarray:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰"""
        if isinstance(texts, str):
            texts = [texts]
        
        # æ£€æŸ¥ç¼“å­˜
        cached_embeddings = []
        uncached_texts = []
        uncached_indices = []
        
        if self.disk_cache:
            for i, text in enumerate(texts):
                text_hash = hashlib.md5(text.encode()).hexdigest()
                cached = self.disk_cache.get(f"emb_{text_hash}")
                if cached is not None:
                    cached_embeddings.append((i, cached))
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)
        else:
            uncached_texts = texts
            uncached_indices = list(range(len(texts)))
        
        # è®¡ç®—æœªç¼“å­˜çš„åµŒå…¥
        all_embeddings = [None] * len(texts)
        
        # å¡«å……ç¼“å­˜çš„ç»“æœ
        for i, emb in cached_embeddings:
            all_embeddings[i] = emb
            self.stats["cache_hits"] += 1
        
        # æ‰¹é‡è®¡ç®—æœªç¼“å­˜çš„åµŒå…¥
        if uncached_texts:
            try:
                if self.model_type == "bge":
                    # BGEæ¨¡å‹æ‰¹é‡å¤„ç†
                    if self.config.enable_batch_processing and len(uncached_texts) > 1:
                        new_embeddings = self.model.encode(
                            uncached_texts,
                            batch_size=self.config.batch_size,
                            max_length=self.config.max_seq_length
                        )['dense_vecs']
                    else:
                        new_embeddings = []
                        for text in uncached_texts:
                            emb = self.model.encode([text])['dense_vecs'][0]
                            new_embeddings.append(emb)
                        new_embeddings = np.array(new_embeddings)
                else:
                    # Sentence-Transformeræ‰¹é‡å¤„ç†
                    new_embeddings = self.model.encode(
                        uncached_texts,
                        batch_size=self.config.batch_size,
                        show_progress_bar=show_progress,
                        convert_to_numpy=True
                    )
                
                # å¡«å……æ–°è®¡ç®—çš„ç»“æœå¹¶ç¼“å­˜
                for i, emb in zip(uncached_indices, new_embeddings):
                    all_embeddings[i] = emb
                    
                    # ç¼“å­˜ç»“æœ
                    if self.disk_cache:
                        text_hash = hashlib.md5(texts[i].encode()).hexdigest()
                        self.disk_cache.set(
                            f"emb_{text_hash}", 
                            emb, 
                            expire=self.config.cache_ttl_hours * 3600
                        )
                
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥è®¡ç®—å¤±è´¥: {e}")
                raise
        
        result = np.array([emb for emb in all_embeddings if emb is not None])
        return result
    
    def build_index(self, knowledge_items: List[Dict], force_rebuild: bool = False) -> bool:
        """æ„å»ºé«˜æ€§èƒ½FAISSç´¢å¼•"""
        try:
            index_cache_path = self.cache_dir / "faiss_index.pkl"
            metadata_cache_path = self.cache_dir / "index_metadata.json"
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åŠ è½½ç¼“å­˜çš„ç´¢å¼•
            if not force_rebuild and index_cache_path.exists() and metadata_cache_path.exists():
                try:
                    logger.info("ğŸ“¥ å°è¯•åŠ è½½ç¼“å­˜çš„ç´¢å¼•...")
                    with open(metadata_cache_path, 'r', encoding='utf-8') as f:
                        cached_metadata = json.load(f)
                    
                    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
                    if cached_metadata.get("total_items") == len(knowledge_items):
                        with open(index_cache_path, 'rb') as f:
                            cache_data = pickle.load(f)
                        
                        self.index = cache_data["index"]
                        self.knowledge_items = cache_data["knowledge_items"]
                        self.id_mapping = cache_data["id_mapping"]
                        
                        logger.success("âœ… æˆåŠŸåŠ è½½ç¼“å­˜ç´¢å¼•")
                        return True
                except Exception as e:
                    logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜ç´¢å¼•å¤±è´¥ï¼Œé‡æ–°æ„å»º: {e}")
            
            # æ„å»ºæ–°ç´¢å¼•
            logger.info(f"ğŸ—ï¸ æ„å»ºFAISSç´¢å¼•ï¼ŒçŸ¥è¯†æ¡ç›®æ•°: {len(knowledge_items)}")
            start_time = time.time()
            
            # å‡†å¤‡æ–‡æœ¬æ•°æ®
            texts = []
            self.knowledge_items = knowledge_items
            self.id_mapping = {}
            
            for i, item in enumerate(knowledge_items):
                # ç»„åˆæ ‡é¢˜å’Œå†…å®¹è¿›è¡ŒåµŒå…¥
                title = item.get('title', '')
                content = item.get('content', item.get('full_content', ''))
                combined_text = f"{title}\n{content}"
                texts.append(combined_text)
                self.id_mapping[i] = item.get('id', str(i))
            
            # æ‰¹é‡è®¡ç®—åµŒå…¥å‘é‡
            logger.info("ğŸ”¢ è®¡ç®—åµŒå…¥å‘é‡...")
            embeddings = self.get_embeddings(texts, show_progress=True)
            
            # é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹
            dimension = embeddings.shape[1]
            num_vectors = embeddings.shape[0]
            
            logger.info(f"ğŸ“Š å‘é‡ç»´åº¦: {dimension}, å‘é‡æ•°é‡: {num_vectors}")
            
            if self.config.index_type == "auto":
                if num_vectors < 1000:
                    index_type = "flat"
                elif num_vectors < 10000:
                    index_type = "ivf"
                else:
                    index_type = "hnsw"
            else:
                index_type = self.config.index_type
            
            # åˆ›å»ºFAISSç´¢å¼•
            if index_type == "flat":
                # ç²¾ç¡®æœç´¢ï¼Œå°æ•°æ®é›†æœ€ä½³
                self.index = faiss.IndexFlatIP(dimension)
                logger.info("ğŸ¯ ä½¿ç”¨ç²¾ç¡®æœç´¢ç´¢å¼•(Flat)")
                
            elif index_type == "ivf":
                # è¿‘ä¼¼æœç´¢ï¼Œä¸­ç­‰æ•°æ®é›†
                nlist = min(self.config.nlist, num_vectors // 10)
                quantizer = faiss.IndexFlatIP(dimension)
                self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
                self.index.train(embeddings.astype(np.float32))
                logger.info(f"âš¡ ä½¿ç”¨IVFç´¢å¼•ï¼Œèšç±»æ•°: {nlist}")
                
            elif index_type == "hnsw":
                # HNSWç´¢å¼•ï¼Œå¤§æ•°æ®é›†é«˜æ€§èƒ½
                self.index = faiss.IndexHNSWFlat(dimension, self.config.M)
                self.index.hnsw.efConstruction = self.config.efConstruction
                logger.info(f"ğŸš€ ä½¿ç”¨HNSWç´¢å¼•ï¼ŒM: {self.config.M}, efConstruction: {self.config.efConstruction}")
            
            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            logger.info("ğŸ“¥ æ·»åŠ å‘é‡åˆ°ç´¢å¼•...")
            self.index.add(embeddings.astype(np.float32))
            
            # ç¼“å­˜ç´¢å¼•
            cache_data = {
                "index": self.index,
                "knowledge_items": self.knowledge_items,
                "id_mapping": self.id_mapping
            }
            
            with open(index_cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            metadata = {
                "total_items": len(knowledge_items),
                "dimension": dimension,
                "index_type": index_type,
                "build_time": time.time()
            }
            
            with open(metadata_cache_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            build_time = time.time() - start_time
            self.stats["index_size"] = num_vectors
            
            logger.success(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
            logger.info(f"â±ï¸ æ„å»ºè€—æ—¶: {build_time:.2f}ç§’")
            logger.info(f"ğŸ“Š ç´¢å¼•ç±»å‹: {index_type}")
            logger.info(f"ğŸ’¾ ç´¢å¼•å¤§å°: {num_vectors} å‘é‡")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False
    
    def search(self, query: str, top_k: int = None, threshold: float = None) -> List[SearchResult]:
        """é«˜æ€§èƒ½è¯­ä¹‰æœç´¢"""
        if not self.model or not self.index:
            logger.error("âŒ æ¨¡å‹æˆ–ç´¢å¼•æœªåˆå§‹åŒ–")
            return []
        
        top_k = top_k or self.config.max_results
        threshold = threshold or self.config.similarity_threshold
        
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æŸ¥è¯¢ç»“æœç¼“å­˜
            query_hash = hashlib.md5(f"{query}_{top_k}_{threshold}".encode()).hexdigest()
            
            if self.result_cache:
                cached_result = self.result_cache.get(f"search_{query_hash}")
                if cached_result:
                    self.stats["cache_hits"] += 1
                    logger.info(f"ğŸ’¨ ç¼“å­˜å‘½ä¸­ï¼ŒæŸ¥è¯¢: '{query[:30]}...'")
                    return cached_result
            
            # è®¡ç®—æŸ¥è¯¢å‘é‡
            query_embedding = self.get_embeddings([query])[0]
            
            # æ‰§è¡Œå‘é‡æœç´¢
            scores, indices = self.index.search(
                query_embedding.reshape(1, -1).astype(np.float32), 
                top_k * 2  # æœç´¢æ›´å¤šç»“æœåç­›é€‰
            )
            
            # å¤„ç†æœç´¢ç»“æœ
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx == -1 or score < threshold:
                    continue
                
                knowledge_item = self.knowledge_items[idx]
                knowledge_id = self.id_mapping.get(idx, str(idx))
                
                # æå–ç›¸å…³ç‰‡æ®µ
                content_snippet = self._extract_relevant_snippet(
                    knowledge_item.get('content', ''),
                    query,
                    max_length=self.config.chunk_size
                )
                
                result = SearchResult(
                    knowledge_id=knowledge_id,
                    title=knowledge_item.get('title', ''),
                    content_snippet=content_snippet,
                    similarity_score=float(score),
                    source_type='semantic',
                    metadata={
                        'index': int(idx),
                        'search_time': time.time() - start_time,
                        'model': self.config.embedding_model
                    },
                    full_content=knowledge_item.get('content', '')
                )
                results.append(result)
            
            # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
            results = results[:top_k]
            
            # ç¼“å­˜ç»“æœ
            if self.result_cache:
                self.result_cache.set(
                    f"search_{query_hash}",
                    results,
                    expire=self.config.cache_ttl_hours * 3600
                )
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["total_searches"] += 1
            search_time = time.time() - start_time
            self.stats["avg_search_time"] = (
                (self.stats["avg_search_time"] * (self.stats["total_searches"] - 1) + search_time) /
                self.stats["total_searches"]
            )
            
            logger.info(f"ğŸ” è¯­ä¹‰æœç´¢å®Œæˆ: '{query[:30]}...' â†’ {len(results)}ä¸ªç»“æœï¼Œè€—æ—¶: {search_time:.3f}ç§’")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_relevant_snippet(self, content: str, query: str, max_length: int = 300) -> str:
        """æå–æœ€ç›¸å…³çš„å†…å®¹ç‰‡æ®µ"""
        if not content or len(content) <= max_length:
            return content
        
        # ç®€å•å®ç°ï¼šæŸ¥æ‰¾åŒ…å«æŸ¥è¯¢è¯çš„æ®µè½
        sentences = content.split('ã€‚')
        best_sentence = ""
        max_score = 0
        
        query_words = set(query.lower().split())
        
        for sentence in sentences:
            if len(sentence.strip()) < 10:
                continue
                
            sentence_words = set(sentence.lower().split())
            score = len(query_words & sentence_words)
            
            if score > max_score:
                max_score = score
                best_sentence = sentence
        
        if best_sentence:
            # æ‰©å±•ä¸Šä¸‹æ–‡
            start_idx = max(0, content.find(best_sentence) - 50)
            end_idx = min(len(content), start_idx + max_length)
            return content[start_idx:end_idx]
        
        # å›é€€ï¼šè¿”å›å¼€å¤´éƒ¨åˆ†
        return content[:max_length]
    
    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "cache_hit_rate": self.stats["cache_hits"] / max(1, self.stats["total_searches"]),
            "memory_usage_mb": psutil.Process().memory_info().rss / 1024 / 1024
        }
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if self.disk_cache:
            self.disk_cache.clear()
        if self.result_cache:
            self.result_cache.clear()
        logger.info("ğŸ§¹ ç¼“å­˜å·²æ¸…ç†")

# å·¥å‚å‡½æ•°
def create_semantic_search_engine(config_dict: Dict) -> HighPerformanceSemanticSearch:
    """åˆ›å»ºè¯­ä¹‰æœç´¢å¼•æ“å®ä¾‹"""
    config = SearchConfig(**config_dict)
    engine = HighPerformanceSemanticSearch(config)
    
    if not engine.initialize_model():
        raise RuntimeError("è¯­ä¹‰æœç´¢å¼•æ“åˆå§‹åŒ–å¤±è´¥")
    
    return engine 