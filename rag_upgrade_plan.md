# ğŸ§  RAGèƒ½åŠ›å‡çº§è®¡åˆ’ v2.0

## ğŸ¯ å‡çº§ç›®æ ‡

### å½“å‰èƒ½åŠ›
```
âœ… åŸºç¡€å…³é”®è¯åŒ¹é…
âœ… Notionå®æ—¶æŸ¥è¯¢
âœ… ä½¿ç”¨é¢‘ç‡ç»Ÿè®¡
âœ… ç®€å•ç‰‡æ®µæå–
```

### ç›®æ ‡èƒ½åŠ›
```
ğŸš€ è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
ğŸš€ ä¸Šä¸‹æ–‡ç†è§£
ğŸš€ å¤šè½®å¯¹è¯æ”¯æŒ
ğŸš€ æ™ºèƒ½çŸ¥è¯†æ¨ç†
ğŸš€ è‡ªåŠ¨çŸ¥è¯†æ›´æ–°å»ºè®®
```

---

## ğŸ“‹ æŠ€æœ¯å®æ–½è·¯å¾„

### Phase 1: è¯­ä¹‰æ£€ç´¢å¢å¼ºï¼ˆç«‹å³å¯åšï¼Œ1å‘¨å†…å®Œæˆï¼‰

#### 1.1 é›†æˆSentence Transformers
```python
# æ–°å¢æ–‡ä»¶: semantic_search.py
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import os

class SemanticSearchEngine:
    def __init__(self, model_name="shibing624/text2vec-base-chinese"):
        self.model = SentenceTransformer(model_name)
        self.knowledge_embeddings = None
        self.knowledge_texts = []
        self.embedding_cache_file = "knowledge_embeddings.pkl"
    
    def build_knowledge_index(self, knowledge_items):
        """æ„å»ºçŸ¥è¯†åº“å‘é‡ç´¢å¼•"""
        print("ğŸ”„ æ„å»ºçŸ¥è¯†åº“å‘é‡ç´¢å¼•...")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = []
        for item in knowledge_items:
            # ç»„åˆæ ‡é¢˜ã€å…³é”®è¯å’Œå†…å®¹æ‘˜è¦
            combined_text = f"{item['title']} {' '.join(item['keywords'])} {item['content'][:500]}"
            texts.append(combined_text)
        
        self.knowledge_texts = texts
        self.knowledge_embeddings = self.model.encode(texts)
        
        # ç¼“å­˜embeddings
        self._save_embeddings()
        print(f"âœ… çŸ¥è¯†åº“ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(texts)} ä¸ªæ¡ç›®")
    
    def semantic_search(self, query, top_k=5, similarity_threshold=0.3):
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢"""
        if self.knowledge_embeddings is None:
            self._load_embeddings()
        
        query_embedding = self.model.encode([query])
        similarities = cosine_similarity(query_embedding, self.knowledge_embeddings)[0]
        
        # ç­›é€‰è¶…è¿‡é˜ˆå€¼çš„ç»“æœ
        valid_indices = np.where(similarities > similarity_threshold)[0]
        valid_similarities = similarities[valid_indices]
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        sorted_indices = np.argsort(valid_similarities)[::-1][:top_k]
        
        results = []
        for idx in sorted_indices:
            original_idx = valid_indices[idx]
            results.append({
                'index': original_idx,
                'similarity': valid_similarities[idx],
                'text': self.knowledge_texts[original_idx]
            })
        
        return results
    
    def _save_embeddings(self):
        """ä¿å­˜embeddingsåˆ°æœ¬åœ°"""
        cache_data = {
            'embeddings': self.knowledge_embeddings,
            'texts': self.knowledge_texts
        }
        with open(self.embedding_cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
    
    def _load_embeddings(self):
        """ä»æœ¬åœ°åŠ è½½embeddings"""
        if os.path.exists(self.embedding_cache_file):
            with open(self.embedding_cache_file, 'rb') as f:
                cache_data = pickle.load(f)
                self.knowledge_embeddings = cache_data['embeddings']
                self.knowledge_texts = cache_data['texts']
```

#### 1.2 é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
```python
# ä¿®æ”¹ notion_knowledge_db.py
class NotionKnowledgeDB(NotionHandler):
    def __init__(self, config):
        super().__init__(config)
        # æ–°å¢è¯­ä¹‰æœç´¢å¼•æ“
        self.semantic_engine = None
        self._init_semantic_search()
    
    def _init_semantic_search(self):
        """åˆå§‹åŒ–è¯­ä¹‰æœç´¢å¼•æ“"""
        try:
            from semantic_search import SemanticSearchEngine
            self.semantic_engine = SemanticSearchEngine()
            print("âœ… è¯­ä¹‰æœç´¢å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            print("âš ï¸  è¯­ä¹‰æœç´¢ä¾èµ–æœªå®‰è£…ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢")
    
    def enhanced_search_knowledge(self, query, use_semantic=True):
        """å¢å¼ºç‰ˆçŸ¥è¯†æœç´¢"""
        results = []
        
        # 1. ä¼ ç»Ÿå…³é”®è¯æœç´¢
        keyword_results = self.search_knowledge_by_keywords([query])
        results.extend([{'source': 'keyword', 'data': item} for item in keyword_results])
        
        # 2. è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
        if use_semantic and self.semantic_engine:
            # è·å–æ‰€æœ‰çŸ¥è¯†ç”¨äºè¯­ä¹‰æœç´¢
            all_knowledge = self._get_all_knowledge()
            if all_knowledge:
                self.semantic_engine.build_knowledge_index(all_knowledge)
                semantic_results = self.semantic_engine.semantic_search(query)
                
                for result in semantic_results:
                    knowledge_item = all_knowledge[result['index']]
                    knowledge_item['semantic_score'] = result['similarity']
                    results.append({'source': 'semantic', 'data': knowledge_item})
        
        # 3. åˆå¹¶å’Œå»é‡
        unique_results = self._merge_and_deduplicate(results)
        
        # 4. é‡æ–°æ’åºï¼ˆä¼˜å…ˆçº§ + è¯­ä¹‰ç›¸ä¼¼åº¦ + ä½¿ç”¨é¢‘ç‡ï¼‰
        sorted_results = self._smart_ranking(unique_results, query)
        
        return sorted_results[:5]  # è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„
```

### Phase 2: æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆ2å‘¨å†…å®Œæˆï¼‰

#### 2.1 æ™ºèƒ½ç‰‡æ®µæå–
```python
# æ–°å¢æ–‡ä»¶: smart_chunking.py
import re
from typing import List, Dict

class SmartChunking:
    def __init__(self, chunk_size=300, overlap=50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def extract_relevant_snippets(self, content: str, query: str, max_snippets=3):
        """æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„å†…å®¹ç‰‡æ®µ"""
        # 1. æŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_by_semantics(content)
        
        # 2. è®¡ç®—æ¯ä¸ªæ®µè½ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        paragraph_scores = []
        for para in paragraphs:
            score = self._calculate_relevance(para, query)
            paragraph_scores.append((para, score))
        
        # 3. é€‰æ‹©æœ€ç›¸å…³çš„æ®µè½
        paragraph_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 4. ç”Ÿæˆè¿è´¯çš„ç‰‡æ®µ
        snippets = []
        for para, score in paragraph_scores[:max_snippets]:
            if score > 0.1:  # ç›¸å…³æ€§é˜ˆå€¼
                snippet = self._create_contextual_snippet(para, content)
                snippets.append({
                    'content': snippet,
                    'relevance': score,
                    'type': 'semantic_match'
                })
        
        return snippets
    
    def _split_by_semantics(self, content: str) -> List[str]:
        """æŒ‰è¯­ä¹‰åˆ†å‰²å†…å®¹"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        # è¿›ä¸€æ­¥æŒ‰å¥å­åˆ†å‰²é•¿æ®µè½
        refined_paragraphs = []
        for para in paragraphs:
            if len(para) > self.chunk_size:
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', para)
                current_chunk = ""
                for sentence in sentences:
                    if len(current_chunk + sentence) < self.chunk_size:
                        current_chunk += sentence
                    else:
                        if current_chunk:
                            refined_paragraphs.append(current_chunk)
                        current_chunk = sentence
                if current_chunk:
                    refined_paragraphs.append(current_chunk)
            else:
                refined_paragraphs.append(para)
        
        return [p.strip() for p in refined_paragraphs if p.strip()]
    
    def _calculate_relevance(self, paragraph: str, query: str) -> float:
        """è®¡ç®—æ®µè½ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"""
        # ç®€å•çš„å…³é”®è¯é‡å è®¡ç®—
        query_words = set(query.lower().split())
        para_words = set(paragraph.lower().split())
        
        if not query_words:
            return 0
        
        overlap = len(query_words.intersection(para_words))
        return overlap / len(query_words)
```

#### 2.2 ä¸Šä¸‹æ–‡æ„ŸçŸ¥
```python
# æ–°å¢æ–‡ä»¶: context_analyzer.py
class ContextAnalyzer:
    def __init__(self):
        self.conversation_history = []
    
    def analyze_query_context(self, current_query: str, history: List[str] = None):
        """åˆ†ææŸ¥è¯¢çš„ä¸Šä¸‹æ–‡"""
        if history:
            self.conversation_history = history
        
        # 1. è¯†åˆ«æŸ¥è¯¢ç±»å‹
        query_type = self._identify_query_type(current_query)
        
        # 2. æå–å…³é”®å®ä½“
        entities = self._extract_entities(current_query)
        
        # 3. åˆ†æå¯¹è¯æ„å›¾
        intent = self._analyze_intent(current_query, self.conversation_history)
        
        # 4. ç”Ÿæˆå¢å¼ºæŸ¥è¯¢
        enhanced_query = self._enhance_query(current_query, entities, intent)
        
        return {
            'original_query': current_query,
            'enhanced_query': enhanced_query,
            'query_type': query_type,
            'entities': entities,
            'intent': intent,
            'context_keywords': self._extract_context_keywords()
        }
    
    def _identify_query_type(self, query: str) -> str:
        """è¯†åˆ«æŸ¥è¯¢ç±»å‹"""
        question_patterns = {
            'what': ['ä»€ä¹ˆ', 'æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯'],
            'how': ['æ€ä¹ˆ', 'å¦‚ä½•', 'æ€æ ·'],
            'why': ['ä¸ºä»€ä¹ˆ', 'ä¸ºä½•'],
            'when': ['ä»€ä¹ˆæ—¶å€™', 'ä½•æ—¶'],
            'where': ['å“ªé‡Œ', 'åœ¨å“ª'],
            'who': ['è°', 'ä»€ä¹ˆäºº']
        }
        
        for qtype, patterns in question_patterns.items():
            if any(pattern in query for pattern in patterns):
                return qtype
        
        return 'general'
```

### Phase 3: çŸ¥è¯†å›¾è°±é›†æˆï¼ˆ1ä¸ªæœˆå†…å®Œæˆï¼‰

#### 3.1 çŸ¥è¯†å…³ç³»å»ºæ¨¡
```python
# æ–°å¢æ–‡ä»¶: knowledge_graph.py
import networkx as nx
from typing import Dict, List, Tuple

class KnowledgeGraph:
    def __init__(self):
        self.graph = nx.DiGraph()
        self.entity_embeddings = {}
    
    def build_from_notion_data(self, knowledge_items: List[Dict]):
        """ä»Notionæ•°æ®æ„å»ºçŸ¥è¯†å›¾è°±"""
        # 1. æ·»åŠ çŸ¥è¯†èŠ‚ç‚¹
        for item in knowledge_items:
            self.graph.add_node(
                item['id'],
                title=item['title'],
                category=item['category'],
                keywords=item['keywords'],
                content=item['content']
            )
        
        # 2. åŸºäºå…³é”®è¯å»ºç«‹å…³ç³»
        self._build_keyword_relations(knowledge_items)
        
        # 3. åŸºäºåˆ†ç±»å»ºç«‹å…³ç³»
        self._build_category_relations(knowledge_items)
        
        # 4. åŸºäºå†…å®¹ç›¸ä¼¼åº¦å»ºç«‹å…³ç³»
        self._build_semantic_relations(knowledge_items)
    
    def find_related_knowledge(self, knowledge_id: str, relation_types: List[str] = None, max_depth=2):
        """æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†"""
        if knowledge_id not in self.graph:
            return []
        
        related = []
        
        # å¹¿åº¦ä¼˜å…ˆæœç´¢ç›¸å…³èŠ‚ç‚¹
        visited = set()
        queue = [(knowledge_id, 0)]
        
        while queue:
            node_id, depth = queue.pop(0)
            
            if depth >= max_depth or node_id in visited:
                continue
            
            visited.add(node_id)
            
            # è·å–é‚»å±…èŠ‚ç‚¹
            for neighbor in self.graph.neighbors(node_id):
                edge_data = self.graph[node_id][neighbor]
                relation_type = edge_data.get('relation_type')
                
                if relation_types is None or relation_type in relation_types:
                    related.append({
                        'id': neighbor,
                        'relation_type': relation_type,
                        'strength': edge_data.get('strength', 0),
                        'depth': depth + 1
                    })
                    
                    queue.append((neighbor, depth + 1))
        
        # æŒ‰å…³ç³»å¼ºåº¦æ’åº
        related.sort(key=lambda x: x['strength'], reverse=True)
        return related[:10]  # è¿”å›å‰10ä¸ªæœ€ç›¸å…³çš„
```

### Phase 4: å¤§æ¨¡å‹é›†æˆï¼ˆ2-3ä¸ªæœˆï¼‰

#### 4.1 æ™ºèƒ½æ¨ç†å¢å¼º
```python
# æ–°å¢æ–‡ä»¶: llm_enhanced_rag.py
class LLMEnhancedRAG:
    def __init__(self, llm_handler):
        self.llm = llm_handler
        self.knowledge_db = None
        self.semantic_engine = None
        self.knowledge_graph = None
    
    def intelligent_knowledge_retrieval(self, query: str, context: Dict = None):
        """æ™ºèƒ½çŸ¥è¯†æ£€ç´¢ä¸æ¨ç†"""
        # 1. æŸ¥è¯¢åˆ†æå’Œå¢å¼º
        analyzed_query = self._analyze_and_enhance_query(query, context)
        
        # 2. å¤šç­–ç•¥çŸ¥è¯†æ£€ç´¢
        knowledge_candidates = self._multi_strategy_retrieval(analyzed_query)
        
        # 3. çŸ¥è¯†æ¨ç†å’Œæ•´åˆ
        integrated_knowledge = self._integrate_and_reason(knowledge_candidates, query)
        
        # 4. åŠ¨æ€ä¸Šä¸‹æ–‡ç”Ÿæˆ
        final_context = self._generate_dynamic_context(integrated_knowledge, query)
        
        return final_context
    
    def _analyze_and_enhance_query(self, query: str, context: Dict) -> Dict:
        """ä½¿ç”¨LLMåˆ†æå’Œå¢å¼ºæŸ¥è¯¢"""
        analysis_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œå…³é”®ä¿¡æ¯ï¼š
        
        æŸ¥è¯¢: {query}
        ä¸Šä¸‹æ–‡: {context or 'æ— '}
        
        è¯·æä¾›ï¼š
        1. æŸ¥è¯¢æ„å›¾åˆ†ç±»
        2. å…³é”®å®ä½“æå–
        3. å¯èƒ½çš„ç›¸å…³æ¦‚å¿µ
        4. å»ºè®®çš„æœç´¢å…³é”®è¯
        
        ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚
        """
        
        response = self.llm.generate_response(analysis_prompt)
        return self._parse_analysis_response(response)
    
    def _multi_strategy_retrieval(self, analyzed_query: Dict) -> List[Dict]:
        """å¤šç­–ç•¥çŸ¥è¯†æ£€ç´¢"""
        all_results = []
        
        # ç­–ç•¥1: å…³é”®è¯ç²¾ç¡®åŒ¹é…
        keyword_results = self.knowledge_db.search_knowledge_by_keywords(
            analyzed_query['search_keywords']
        )
        all_results.extend([{'source': 'keyword', 'data': r} for r in keyword_results])
        
        # ç­–ç•¥2: è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
        semantic_results = self.semantic_engine.semantic_search(
            analyzed_query['enhanced_query']
        )
        all_results.extend([{'source': 'semantic', 'data': r} for r in semantic_results])
        
        # ç­–ç•¥3: çŸ¥è¯†å›¾è°±å…³ç³»æŸ¥æ‰¾
        if analyzed_query.get('entities'):
            graph_results = self._graph_based_search(analyzed_query['entities'])
            all_results.extend([{'source': 'graph', 'data': r} for r in graph_results])
        
        return all_results
    
    def _integrate_and_reason(self, knowledge_candidates: List[Dict], query: str) -> Dict:
        """çŸ¥è¯†æ•´åˆå’Œæ¨ç†"""
        integration_prompt = f"""
        åŸºäºä»¥ä¸‹çŸ¥è¯†ç‰‡æ®µï¼Œä¸ºç”¨æˆ·æŸ¥è¯¢æä¾›æœ€ç›¸å…³å’Œå‡†ç¡®çš„ä¿¡æ¯ï¼š
        
        ç”¨æˆ·æŸ¥è¯¢: {query}
        
        å¯ç”¨çŸ¥è¯†:
        {self._format_knowledge_for_prompt(knowledge_candidates)}
        
        è¯·ï¼š
        1. ç­›é€‰æœ€ç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µ
        2. æ•´åˆäº’è¡¥ä¿¡æ¯
        3. è§£å†³å¯èƒ½çš„å†²çª
        4. ç”Ÿæˆç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡
        
        è¿”å›æ•´åˆåçš„çŸ¥è¯†ä¸Šä¸‹æ–‡ã€‚
        """
        
        integrated_response = self.llm.generate_response(integration_prompt)
        return self._parse_integration_response(integrated_response)
```

---

## ğŸ“… å®æ–½æ—¶é—´è¡¨

### ç¬¬1å‘¨ï¼šåŸºç¡€å¢å¼º
```
âœ… å®‰è£…sentence-transformers
âœ… å®ç°è¯­ä¹‰æœç´¢åŸºç¡€åŠŸèƒ½
âœ… é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
âœ… æœ¬åœ°æµ‹è¯•éªŒè¯
```

### ç¬¬2-3å‘¨ï¼šæ··åˆæ£€ç´¢
```
ğŸ”„ æ™ºèƒ½ç‰‡æ®µæå–
ğŸ”„ ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æ
ğŸ”„ å¤šç­–ç•¥ç»“æœåˆå¹¶
ğŸ”„ äº‘ç«¯éƒ¨ç½²æµ‹è¯•
```

### ç¬¬4-6å‘¨ï¼šå›¾è°±é›†æˆ
```
ğŸ”„ çŸ¥è¯†å…³ç³»å»ºæ¨¡
ğŸ”„ å›¾è°±æ„å»ºç®—æ³•
ğŸ”„ å…³ç³»æ¨ç†åŠŸèƒ½
ğŸ”„ å¯è§†åŒ–ç®¡ç†ç•Œé¢
```

### ç¬¬7-12å‘¨ï¼šLLMå¢å¼º
```
ğŸ”„ æŸ¥è¯¢åˆ†æå¢å¼º
ğŸ”„ çŸ¥è¯†æ¨ç†é›†æˆ
ğŸ”„ åŠ¨æ€ä¸Šä¸‹æ–‡ç”Ÿæˆ
ğŸ”„ è‡ªå­¦ä¹ ä¼˜åŒ–æœºåˆ¶
```

---

## ğŸ› ï¸ æŠ€æœ¯ä¾èµ–å’Œéƒ¨ç½²

### æ–°å¢ä¾èµ–åŒ…
```bash
# æ ¸å¿ƒä¾èµ–
pip install sentence-transformers
pip install scikit-learn
pip install numpy
pip install networkx

# å¯é€‰å¢å¼º
pip install faiss-cpu  # å‘é‡ç´¢å¼•åŠ é€Ÿ
pip install jieba      # ä¸­æ–‡åˆ†è¯
pip install transformers  # é¢„è®­ç»ƒæ¨¡å‹æ”¯æŒ
```

### äº‘ç«¯éƒ¨ç½²è€ƒè™‘
```dockerfile
# æ›´æ–°Dockerfile
FROM python:3.9-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# ... ç°æœ‰é…ç½® ...

# æ–°å¢RAGä¾èµ–
COPY requirements_rag.txt .
RUN pip install --no-cache-dir -r requirements_rag.txt

# é¢„ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼Œå‡å°‘å¯åŠ¨æ—¶é—´ï¼‰
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('shibing624/text2vec-base-chinese')"
```

### é…ç½®æ–‡ä»¶æ‰©å±•
```json
{
  "rag_config": {
    "enable_semantic_search": true,
    "embedding_model": "shibing624/text2vec-base-chinese",
    "similarity_threshold": 0.3,
    "max_semantic_results": 5,
    "enable_knowledge_graph": true,
    "enable_llm_reasoning": false,
    "chunk_size": 300,
    "chunk_overlap": 50
  }
}
```

---

## ğŸ¯ é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡ç›®æ ‡
```
å½“å‰ â†’ ç›®æ ‡

åŒ¹é…å‡†ç¡®ç‡: 70% â†’ 90%
å“åº”ç›¸å…³æ€§: 60% â†’ 85%
ä¸Šä¸‹æ–‡è´¨é‡: ä¸­ç­‰ â†’ ä¼˜ç§€
å¤šè½®å¯¹è¯: ä¸æ”¯æŒ â†’ å®Œå…¨æ”¯æŒ
æ¨ç†èƒ½åŠ›: æ—  â†’ åŸºç¡€æ¨ç†
è‡ªå­¦ä¹ èƒ½åŠ›: æ—  â†’ æŒç»­ä¼˜åŒ–
```

### ç”¨æˆ·ä½“éªŒæ”¹å–„
```
âœ… æ¨¡ç³ŠæŸ¥è¯¢ä¹Ÿèƒ½ç²¾å‡†åŒ¹é…
âœ… æ”¯æŒè‡ªç„¶è¯­è¨€æè¿°é—®é¢˜
âœ… å¤šè½®å¯¹è¯ç†è§£ä¸Šä¸‹æ–‡
âœ… è‡ªåŠ¨è¡¥å……ç›¸å…³çŸ¥è¯†
âœ… æ™ºèƒ½æ¨ç†å’Œè§£é‡Š
```

---

*å‡çº§è®¡åˆ’ç‰ˆæœ¬: v2.0*  
*åˆ¶å®šæ—¶é—´: 2025-01-20*  
*é¢„è®¡å®Œæˆ: 2025-04-20* 